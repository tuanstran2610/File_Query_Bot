# # from huggingface_hub import snapshot_download
# #
# # snapshot_download(
# #     repo_id="trong269/vit5-vietnamese-text-summarization",
# #     local_dir="vit5_model",  # Folder where the model will be saved
# #     local_dir_use_symlinks=False  # Optional: Avoid symbolic links
# # )
#
#
# # from transformers import pipeline
# # from langchain_huggingface import HuggingFacePipeline, HuggingFaceEmbeddings
# # from qdrant_client import QdrantClient
# # from langchain_qdrant import Qdrant
# # from langchain.chains import RetrievalQA
# #
# # qdrant_client = QdrantClient(
# #     url="http://localhost:6333",  # default local Qdrant url
# #     prefer_grpc=False
# # )
# # embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
# #
# # vectorstore = Qdrant(
# #     client=qdrant_client,
# #     collection_name="collections",  # replace with your actual Qdrant collection name
# #     embeddings=embedding_model
# # )
# #
# # summarizer = pipeline(
# #     "text2text-generation",
# #     model="trong269/vit5-vietnamese-text-summarization",
# #     max_length=256,  # allow longer summaries
# #     do_sample=True,
# #     temperature=0.7,
# #     top_p=0.9,
# # )
# #
# # llm = HuggingFacePipeline(pipeline=summarizer)
# # qa_chain = RetrievalQA.from_chain_type(
# #     llm=llm,
# #     chain_type="stuff",  # simplest; can try "map_reduce" or "refine" for better summaries
# #     retriever=vectorstore.as_retriever()
# # )
# #
# # print("Chatbot is ready! Type 'exit' to quit.")
# #
# # while True:
# #     query = input("You: ")
# #     if query.lower() == "exit":
# #         break
# #
# #     answer = qa_chain.run(query)
# #     print("Bot:", answer)
#
# # Load model directly
# # from langchain_huggingface import HuggingFaceEmbeddings
# # from langchain_qdrant import Qdrant
# # from qdrant_client import QdrantClient
# # from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
# #
# # # 1. Thi·∫øt l·∫≠p embedding v√† Qdrant
# # embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
# # qdrant_client = QdrantClient(url="http://localhost:6333")
# #
# # vector_store = Qdrant(
# #     client=qdrant_client,
# #     collection_name="collections",  # ‚ö†Ô∏è Thay b·∫±ng t√™n collection th·ª±c t·∫ø
# #     embeddings=embedding_model,
# #     content_payload_key="page_content"  # ‚ö†Ô∏è Thay b·∫±ng key ƒë√∫ng v·ªõi payload trong Qdrant
# # )
# #
# # # 2. Load m√¥ h√¨nh ViT5
# # model_name = "VietAI/vit5-base"
# # tokenizer = AutoTokenizer.from_pretrained(model_name)
# # model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
# #
# # # 3. V√≤ng l·∫∑p ƒë·ªÉ nh·∫≠p c√¢u h·ªèi t·ª´ ng∆∞·ªùi d√πng
# # while True:
# #     query = input("\nüü° Nh·∫≠p c√¢u h·ªèi (ho·∫∑c g√µ 'exit' ƒë·ªÉ tho√°t): ").strip()
# #     if query.lower() in ['exit', 'quit']:
# #         print("üëã K·∫øt th√∫c.")
# #         break
# #
# #     # Truy v·∫•n Qdrant
# #     retrieved_docs = vector_store.similarity_search(query, k=5)
# #     context = " ".join([doc.page_content for doc in retrieved_docs if doc.page_content])
# #
# #     if not context:
# #         print("‚ùå Kh√¥ng t√¨m th·∫•y n·ªôi dung li√™n quan trong Qdrant.")
# #         continue
# #
# #     # T√≥m t·∫Øt context
# #     input_text = f"summarize: {context}"
# #     inputs = tokenizer(input_text, return_tensors="pt", max_length=512, truncation=True)
# #     summary_ids = model.generate(
# #         inputs["input_ids"],
# #         max_length=150,
# #         min_length=30,
# #         length_penalty=2.0,
# #         num_beams=4,
# #         early_stopping=True
# #     )
# #     summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
# #
# #     # In k·∫øt qu·∫£
# #     print("\nüü© C√¢u h·ªèi:", query)
# #     print("üü¶ T√≥m t·∫Øt:", summary)
#
#
# # import asyncio
# # from langchain_community.embeddings import HuggingFaceEmbeddings
# # from langchain_qdrant import QdrantVectorStore
# # from qdrant_client import QdrantClient
# # from qdrant_client.http.models import Distance, VectorParams
# # from langchain_core.documents import Document
# # from langchain_ollama import OllamaLLM
# #
# #
# # async def setup_qdrant():
# #     try:
# #         # Kh·ªüi t·∫°o embedding model
# #         embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
# #
# #         # K·∫øt n·ªëi v·ªõi Qdrant
# #         qdrant_client = QdrantClient(url="http://localhost:6333", timeout=5)
# #         print("K·∫øt n·ªëi v·ªõi Qdrant th√†nh c√¥ng!")
# #
# #         # Ki·ªÉm tra v√† t·∫°o collection n·∫øu ch∆∞a t·ªìn t·∫°i
# #         collection_name = "collections"
# #         collections = qdrant_client.get_collections()
# #         if collection_name not in [c.name for c in collections.collections]:
# #             qdrant_client.create_collection(
# #                 collection_name=collection_name,
# #                 vectors_config=VectorParams(size=384, distance=Distance.COSINE)
# #             )
# #             print(f"ƒê√£ t·∫°o collection: {collection_name}")
# #
# #         # Kh·ªüi t·∫°o QdrantVectorStore
# #         vector_store = QdrantVectorStore(
# #             client=qdrant_client,
# #             collection_name=collection_name,
# #             embedding=embedding_model,
# #             content_payload_key="text"
# #         )
# #
# #         # Th√™m d·ªØ li·ªáu m·∫´u v·ªÅ Ch√∫a Nh·∫´n
# #         # sample_docs = [
# #         #     Document(
# #         #         page_content="Ch√∫a Nh·∫´n (The Lord of the Rings) l√† b·ªô ba ti·ªÉu thuy·∫øt gi·∫£ t∆∞·ªüng c·ªßa J.R.R. Tolkien, k·ªÉ v·ªÅ h√†nh tr√¨nh c·ªßa Frodo Baggins, m·ªôt hobbit, ƒë·ªÉ ph√° h·ªßy Chi·∫øc Nh·∫´n T·ªëi Th∆∞·ª£ng, m·ªôt v≈© kh√≠ nguy hi·ªÉm c·ªßa Ch√∫a T·ªÉ H·∫Øc √Åm Sauron. C√¢u chuy·ªán b·∫Øt ƒë·∫ßu ·ªü Shire, n∆°i Frodo nh·∫≠n chi·∫øc nh·∫´n t·ª´ ch√∫ m√¨nh, Bilbo Baggins.",
# #         #         metadata={"id": 1}
# #         #     ),
# #         #     Document(
# #         #         page_content="Frodo c√πng v·ªõi c√°c b·∫°n hobbit Sam, Merry, v√† Pippin ƒë∆∞·ª£c ph√π th·ªßy Gandalf h∆∞·ªõng d·∫´n, b·∫Øt ƒë·∫ßu chuy·∫øn ƒëi ƒë·∫øn Rivendell. T·∫°i ƒë√¢y, m·ªôt H·ªôi Nh·∫´n ƒë∆∞·ª£c th√†nh l·∫≠p, bao g·ªìm Frodo, Sam, Merry, Pippin, Gandalf, Aragorn, Legolas, Gimli, v√† Boromir, v·ªõi nhi·ªám v·ª• ƒë∆∞a nh·∫´n ƒë·∫øn Mordor ƒë·ªÉ ph√° h·ªßy n√≥ trong ng·ªçn n√∫i l·ª≠a Mount Doom.",
# #         #         metadata={"id": 2}
# #         #     ),
# #         #     Document(
# #         #         page_content="H·ªôi Nh·∫´n ƒë·ªëi m·∫∑t v·ªõi nhi·ªÅu nguy hi·ªÉm: b·ªã Nazg√ªl truy ƒëu·ªïi, chi·∫øn ƒë·∫•u v·ªõi l≈© orc ·ªü Moria, v√† s·ª± ph·∫£n b·ªôi c·ªßa Boromir. Sau khi Gandalf ng√£ xu·ªëng ·ªü Moria, H·ªôi Nh·∫´n tan r√£. Frodo v√† Sam ti·∫øp t·ª•c h√†nh tr√¨nh m·ªôt m√¨nh, ƒë∆∞·ª£c d·∫´n ƒë∆∞·ªùng b·ªüi Gollum, m·ªôt sinh v·∫≠t b·ªã √°m ·∫£nh b·ªüi chi·∫øc nh·∫´n.",
# #         #         metadata={"id": 3}
# #         #     ),
# #         #     Document(
# #         #         page_content="Trong khi ƒë√≥, Aragorn, Legolas, v√† Gimli gi√∫p v∆∞∆°ng qu·ªëc Rohan Roswell v√† Gondor ch·ªëng l·∫°i qu√¢n ƒë·ªôi c·ªßa Sauron. Sam v√† Frodo ƒë·ªëi m·∫∑t v·ªõi nhi·ªÅu th·ª≠ th√°ch ·ªü Mordor, bao g·ªìm s·ª± c√°m d·ªó c·ªßa chi·∫øc nh·∫´n v√† s·ª± ph·∫£n b·ªôi c·ªßa Gollum. Cu·ªëi c√πng, Frodo ph√° h·ªßy chi·∫øc nh·∫´n t·∫°i Mount Doom, ƒë√°nh b·∫°i Sauron.",
# #         #         metadata={"id": 4}
# #         #     ),
# #         #     Document(
# #         #         page_content="Tom Bombadil l√† m·ªôt nh√¢n v·∫≠t b√≠ ·∫©n gi√∫p Frodo v√† c√°c hobbit tho√°t kh·ªèi nguy hi·ªÉm ·ªü Old Forest. √îng kh√¥ng b·ªã ·∫£nh h∆∞·ªüng b·ªüi s·ª©c m·∫°nh c·ªßa chi·∫øc nh·∫´n. Aragorn, m·ªôt chi·∫øn binh d≈©ng c·∫£m, l√† ng∆∞·ªùi th·ª´a k·∫ø ngai v√†ng Gondor v√† d·∫´n d·∫Øt c√°c d√¢n t·ªôc t·ª± do ch·ªëng l·∫°i Sauron.",
# #         #         metadata={"id": 5}
# #         #     )
# #         # ]
# #         # vector_store.add_documents(sample_docs)
# #         # print("ƒê√£ th√™m d·ªØ li·ªáu m·∫´u v·ªÅ Ch√∫a Nh·∫´n v√†o Qdrant.")
# #
# #         return vector_store, qdrant_client
# #
# #     except Exception as e:
# #         print(f"L·ªói khi thi·∫øt l·∫≠p Qdrant: {str(e)}")
# #         return None, None
# #
# #
# # async def answer_question_with_llama(vector_store, query):
# #     try:
# #         # Kh·ªüi t·∫°o LLM
# #         llm = OllamaLLM(model="llama3.1:8b")
# #
# #         # T√¨m ki·∫øm t∆∞∆°ng ƒë·ªìng
# #         docs = vector_store.similarity_search(query, k=5)
# #         context = " ".join([doc.page_content for doc in docs if doc.page_content])
# #         if not context:
# #             return "Kh√¥ng t√¨m th·∫•y n·ªôi dung li√™n quan trong Qdrant. Vui l√≤ng cung c·∫•p th√™m th√¥ng tin ho·∫∑c th·ª≠ c√¢u h·ªèi kh√°c."
# #
# #         # Prompt c·∫£i ti·∫øn ƒë·ªÉ tr·∫£ l·ªùi chi ti·∫øt
# #         prompt = (
# #             f"B·∫°n l√† m·ªôt tr·ª£ l√Ω th√¥ng minh, am hi·ªÉu v·ªÅ 'Ch√∫a Nh·∫´n' c·ªßa J.R.R. Tolkien. D·ª±a tr√™n n·ªôi dung sau, h√£y cung c·∫•p c√¢u tr·∫£ l·ªùi chi ti·∫øt, ƒë·∫ßy ƒë·ªß, v√† c√≥ c·∫•u tr√∫c r√µ r√†ng (bao g·ªìm c√°c √Ω ch√≠nh, di·ªÖn bi·∫øn quan tr·ªçng, v√† c√°c nh√¢n v·∫≠t li√™n quan). N·∫øu c√¢u h·ªèi y√™u c·∫ßu t√≥m t·∫Øt chi ti·∫øt, h√£y li·ªát k√™ c√°c s·ª± ki·ªán theo th·ª© t·ª± th·ªùi gian. Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát, s·ª≠ d·ª•ng ng√¥n ng·ªØ t·ª± nhi√™n v√† d·ªÖ hi·ªÉu.\n\n"
# #             f"N·ªôi dung:\n{context}\n\n"
# #             f"C√¢u h·ªèi: {query}\n\n"
# #             f"Tr·∫£ l·ªùi:"
# #         )
# #
# #         # G·ªçi LLM
# #         answer = await asyncio.to_thread(llm.invoke, prompt)
# #         return answer
# #
# #     except Exception as e:
# #         return f"L·ªói khi tr·∫£ l·ªùi c√¢u h·ªèi: {str(e)}"
# #
# #
# # async def main():
# #     # Thi·∫øt l·∫≠p Qdrant v√† vector store
# #     vector_store, qdrant_client = await setup_qdrant()
# #     if not vector_store:
# #         print("Kh√¥ng th·ªÉ ti·∫øp t·ª•c do l·ªói thi·∫øt l·∫≠p Qdrant.")
# #         return
# #
# #     # V√≤ng l·∫∑p truy v·∫•n
# #     while True:
# #         query = input("\nNh·∫≠p c√¢u h·ªèi (ho·∫∑c 'exit' ƒë·ªÉ tho√°t): ").strip()
# #         if query.lower() in ['exit', 'quit']:
# #             break
# #
# #         result = await answer_question_with_llama(vector_store, query)
# #         print("\nüü¶ Tr·∫£ l·ªùi t·ª´ LLaMA:\n", result)
# #
# #     # ƒê√≥ng k·∫øt n·ªëi Qdrant
# #     if qdrant_client:
# #         qdrant_client.close()
# #
# #
# # if __name__ == "__main__":
# #     asyncio.run(main())
#
#
# import asyncio
# from langchain_community.embeddings import HuggingFaceEmbeddings
# from langchain_qdrant import QdrantVectorStore
# from qdrant_client import QdrantClient
# from qdrant_client.http.models import Distance, VectorParams
# from langchain_core.documents import Document
# from transformers import T5Tokenizer, T5ForConditionalGeneration
# import torch
# import sentencepiece  # Ensure SentencePiece is imported to verify installation
#
# async def setup_qdrant():
#     try:
#         # Initialize embedding model
#         embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
#
#         # Connect to Qdrant
#         qdrant_client = QdrantClient(url="http://localhost:6333", timeout=5)
#         print("Connected to Qdrant successfully!")
#
#         # Check and create collection if it doesn't exist
#         collection_name = "collections"
#         collections = qdrant_client.get_collections()
#         if collection_name not in [c.name for c in collections.collections]:
#             qdrant_client.create_collection(
#                 collection_name=collection_name,
#                 vectors_config=VectorParams(size=384, distance=Distance.COSINE)
#             )
#             print(f"Created collection: {collection_name}")
#
#             # Add sample Harry Potter documents for testing
#             vector_store = QdrantVectorStore(
#                 client=qdrant_client,
#                 collection_name=collection_name,
#                 embedding=embedding_model,
#                 content_payload_key="text"
#             )
#
#         else:
#             vector_store = QdrantVectorStore(
#                 client=qdrant_client,
#                 collection_name=collection_name,
#                 embedding=embedding_model,
#                 content_payload_key="text"
#             )
#
#         return vector_store, qdrant_client
#
#     except Exception as e:
#         print(f"Error setting up Qdrant: {str(e)}")
#         return None, None
#
# async def summarize_with_vit5(vector_store, query):
#     try:
#         # Initialize ViT5
#         model_name = "VietAI/vit5-base"
#         try:
#             tokenizer = T5Tokenizer.from_pretrained(model_name)
#             model = T5ForConditionalGeneration.from_pretrained(model_name)
#         except Exception as e:
#             return f"Error loading ViT5 model or tokenizer: {str(e)}. Ensure SentencePiece is installed (`pip install sentencepiece`)."
#
#         device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
#         model.to(device)
#         print(f"ViT5 model loaded on {device}.")
#
#         # Perform similarity search in Qdrant
#         docs = vector_store.similarity_search(query, k=5)
#         context = " ".join([doc.page_content for doc in docs if doc.page_content])
#         if not context:
#             return "No relevant content found in Qdrant. Please add documents or try a different query."
#
#         # Prompt for summarization
#         prompt = (
#             f"T√≥m t·∫Øt c√°c di·ªÖn bi·∫øn ch√≠nh t·ª´ n·ªôi dung sau th√†nh m·ªôt ƒëo·∫°n vƒÉn ng·∫Øn g·ªçn, r√µ r√†ng, theo th·ª© t·ª± th·ªùi gian, s·ª≠ d·ª•ng ng√¥n ng·ªØ ti·∫øng Vi·ªát t·ª± nhi√™n v√† d·ªÖ hi·ªÉu. T·∫≠p trung v√†o c√°c s·ª± ki·ªán quan tr·ªçng v√† nh√¢n v·∫≠t ch√≠nh li√™n quan.\n\n"
#             f"N·ªôi dung: {context}\n\n"
#             f"T√≥m t·∫Øt:"
#         )
#
#         # Prepare input for ViT5
#         inputs = tokenizer(prompt, return_tensors="pt", max_length=512, truncation=True)
#         inputs = inputs.to(device)
#
#         # Generate summary
#         summary_ids = model.generate(
#             inputs["input_ids"],
#             max_length=150,
#             min_length=50,
#             length_penalty=1.0,
#             num_beams=4,
#             early_stopping=True
#         )
#         summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
#
#         return summary
#
#     except Exception as e:
#         return f"Error during ViT5 summarization: {str(e)}"
#
# async def main():
#     # Set up Qdrant and vector store
#     vector_store, qdrant_client = await setup_qdrant()
#     if not vector_store:
#         print("Cannot proceed due to Qdrant setup error.")
#         return
#
#     # Query loop
#     while True:
#         query = input("\nEnter your query (or 'exit' to quit): ").strip()
#         if query.lower() in ['exit', 'quit']:
#             break
#
#         print("\nProcessing query...")
#         result = await summarize_with_vit5(vector_store, query)
#         print("\nüü¶ Summary from ViT5:\n", result)
#
#     # Close Qdrant connection
#     if qdrant_client:
#         qdrant_client.close()
#
# if __name__ == "__main__":
#     asyncio.run(main())


from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

# Load model v√† tokenizer m·ªôt l·∫ßn
tokenizer = AutoTokenizer.from_pretrained("VietAI/vit5-large-vietnews-summarization")
model = AutoModelForSeq2SeqLM.from_pretrained("VietAI/vit5-large-vietnews-summarization")
device = "cuda" if model.device.type == "cuda" else "cpu"
model.to(device)

# V√≤ng l·∫∑p nh·∫≠p input
print("Nh·∫≠p ƒëo·∫°n vƒÉn b·∫£n ti·∫øng Vi·ªát ƒë·ªÉ t√≥m t·∫Øt (g√µ 'exit' ƒë·ªÉ tho√°t):\n")
while True:
    sentence = input(">> Nh·∫≠p vƒÉn b·∫£n: ").strip()
    if sentence.lower() == "exit":
        print("ƒê√£ tho√°t.")
        break
    if not sentence:
        print("‚ùå Vui l√≤ng kh√¥ng ƒë·ªÉ tr·ªëng.")
        continue

    # Chu·∫©n b·ªã ƒë·∫ßu v√†o cho model
    text = "vietnews: " + sentence + " </s>"
    encoding = tokenizer(text, return_tensors="pt").to(device)

    # Sinh t√≥m t·∫Øt
    outputs = model.generate(
        input_ids=encoding["input_ids"],
        attention_mask=encoding["attention_mask"],
        max_length=256,
        early_stopping=True
    )

    # Hi·ªÉn th·ªã k·∫øt qu·∫£
    summary = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)
    print("\nüìù T√≥m t·∫Øt:")
    print(summary)
    print("-" * 80)


# import torch
# print(torch.cuda.is_available())
