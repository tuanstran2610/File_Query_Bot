# # from huggingface_hub import snapshot_download
# #
# # snapshot_download(
# #     repo_id="trong269/vit5-vietnamese-text-summarization",
# #     local_dir="vit5_model",  # Folder where the model will be saved
# #     local_dir_use_symlinks=False  # Optional: Avoid symbolic links
# # )
#
#
# # from transformers import pipeline
# # from langchain_huggingface import HuggingFacePipeline, HuggingFaceEmbeddings
# # from qdrant_client import QdrantClient
# # from langchain_qdrant import Qdrant
# # from langchain.chains import RetrievalQA
# #
# # qdrant_client = QdrantClient(
# #     url="http://localhost:6333",  # default local Qdrant url
# #     prefer_grpc=False
# # )
# # embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
# #
# # vectorstore = Qdrant(
# #     client=qdrant_client,
# #     collection_name="collections",  # replace with your actual Qdrant collection name
# #     embeddings=embedding_model
# # )
# #
# # summarizer = pipeline(
# #     "text2text-generation",
# #     model="trong269/vit5-vietnamese-text-summarization",
# #     max_length=256,  # allow longer summaries
# #     do_sample=True,
# #     temperature=0.7,
# #     top_p=0.9,
# # )
# #
# # llm = HuggingFacePipeline(pipeline=summarizer)
# # qa_chain = RetrievalQA.from_chain_type(
# #     llm=llm,
# #     chain_type="stuff",  # simplest; can try "map_reduce" or "refine" for better summaries
# #     retriever=vectorstore.as_retriever()
# # )
# #
# # print("Chatbot is ready! Type 'exit' to quit.")
# #
# # while True:
# #     query = input("You: ")
# #     if query.lower() == "exit":
# #         break
# #
# #     answer = qa_chain.run(query)
# #     print("Bot:", answer)
#
# # Load model directly
# # from langchain_huggingface import HuggingFaceEmbeddings
# # from langchain_qdrant import Qdrant
# # from qdrant_client import QdrantClient
# # from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
# #
# # # 1. Thiáº¿t láº­p embedding vÃ  Qdrant
# # embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
# # qdrant_client = QdrantClient(url="http://localhost:6333")
# #
# # vector_store = Qdrant(
# #     client=qdrant_client,
# #     collection_name="collections",  # âš ï¸ Thay báº±ng tÃªn collection thá»±c táº¿
# #     embeddings=embedding_model,
# #     content_payload_key="page_content"  # âš ï¸ Thay báº±ng key Ä‘Ãºng vá»›i payload trong Qdrant
# # )
# #
# # # 2. Load mÃ´ hÃ¬nh ViT5
# # model_name = "VietAI/vit5-base"
# # tokenizer = AutoTokenizer.from_pretrained(model_name)
# # model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
# #
# # # 3. VÃ²ng láº·p Ä‘á»ƒ nháº­p cÃ¢u há»i tá»« ngÆ°á»i dÃ¹ng
# # while True:
# #     query = input("\nğŸŸ¡ Nháº­p cÃ¢u há»i (hoáº·c gÃµ 'exit' Ä‘á»ƒ thoÃ¡t): ").strip()
# #     if query.lower() in ['exit', 'quit']:
# #         print("ğŸ‘‹ Káº¿t thÃºc.")
# #         break
# #
# #     # Truy váº¥n Qdrant
# #     retrieved_docs = vector_store.similarity_search(query, k=5)
# #     context = " ".join([doc.page_content for doc in retrieved_docs if doc.page_content])
# #
# #     if not context:
# #         print("âŒ KhÃ´ng tÃ¬m tháº¥y ná»™i dung liÃªn quan trong Qdrant.")
# #         continue
# #
# #     # TÃ³m táº¯t context
# #     input_text = f"summarize: {context}"
# #     inputs = tokenizer(input_text, return_tensors="pt", max_length=512, truncation=True)
# #     summary_ids = model.generate(
# #         inputs["input_ids"],
# #         max_length=150,
# #         min_length=30,
# #         length_penalty=2.0,
# #         num_beams=4,
# #         early_stopping=True
# #     )
# #     summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
# #
# #     # In káº¿t quáº£
# #     print("\nğŸŸ© CÃ¢u há»i:", query)
# #     print("ğŸŸ¦ TÃ³m táº¯t:", summary)
#
#
# # import asyncio
# # from langchain_community.embeddings import HuggingFaceEmbeddings
# # from langchain_qdrant import QdrantVectorStore
# # from qdrant_client import QdrantClient
# # from qdrant_client.http.models import Distance, VectorParams
# # from langchain_core.documents import Document
# # from langchain_ollama import OllamaLLM
# #
# #
# # async def setup_qdrant():
# #     try:
# #         # Khá»Ÿi táº¡o embedding model
# #         embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
# #
# #         # Káº¿t ná»‘i vá»›i Qdrant
# #         qdrant_client = QdrantClient(url="http://localhost:6333", timeout=5)
# #         print("Káº¿t ná»‘i vá»›i Qdrant thÃ nh cÃ´ng!")
# #
# #         # Kiá»ƒm tra vÃ  táº¡o collection náº¿u chÆ°a tá»“n táº¡i
# #         collection_name = "collections"
# #         collections = qdrant_client.get_collections()
# #         if collection_name not in [c.name for c in collections.collections]:
# #             qdrant_client.create_collection(
# #                 collection_name=collection_name,
# #                 vectors_config=VectorParams(size=384, distance=Distance.COSINE)
# #             )
# #             print(f"ÄÃ£ táº¡o collection: {collection_name}")
# #
# #         # Khá»Ÿi táº¡o QdrantVectorStore
# #         vector_store = QdrantVectorStore(
# #             client=qdrant_client,
# #             collection_name=collection_name,
# #             embedding=embedding_model,
# #             content_payload_key="text"
# #         )
# #
# #         # ThÃªm dá»¯ liá»‡u máº«u vá» ChÃºa Nháº«n
# #         # sample_docs = [
# #         #     Document(
# #         #         page_content="ChÃºa Nháº«n (The Lord of the Rings) lÃ  bá»™ ba tiá»ƒu thuyáº¿t giáº£ tÆ°á»Ÿng cá»§a J.R.R. Tolkien, ká»ƒ vá» hÃ nh trÃ¬nh cá»§a Frodo Baggins, má»™t hobbit, Ä‘á»ƒ phÃ¡ há»§y Chiáº¿c Nháº«n Tá»‘i ThÆ°á»£ng, má»™t vÅ© khÃ­ nguy hiá»ƒm cá»§a ChÃºa Tá»ƒ Háº¯c Ãm Sauron. CÃ¢u chuyá»‡n báº¯t Ä‘áº§u á»Ÿ Shire, nÆ¡i Frodo nháº­n chiáº¿c nháº«n tá»« chÃº mÃ¬nh, Bilbo Baggins.",
# #         #         metadata={"id": 1}
# #         #     ),
# #         #     Document(
# #         #         page_content="Frodo cÃ¹ng vá»›i cÃ¡c báº¡n hobbit Sam, Merry, vÃ  Pippin Ä‘Æ°á»£c phÃ¹ thá»§y Gandalf hÆ°á»›ng dáº«n, báº¯t Ä‘áº§u chuyáº¿n Ä‘i Ä‘áº¿n Rivendell. Táº¡i Ä‘Ã¢y, má»™t Há»™i Nháº«n Ä‘Æ°á»£c thÃ nh láº­p, bao gá»“m Frodo, Sam, Merry, Pippin, Gandalf, Aragorn, Legolas, Gimli, vÃ  Boromir, vá»›i nhiá»‡m vá»¥ Ä‘Æ°a nháº«n Ä‘áº¿n Mordor Ä‘á»ƒ phÃ¡ há»§y nÃ³ trong ngá»n nÃºi lá»­a Mount Doom.",
# #         #         metadata={"id": 2}
# #         #     ),
# #         #     Document(
# #         #         page_content="Há»™i Nháº«n Ä‘á»‘i máº·t vá»›i nhiá»u nguy hiá»ƒm: bá»‹ NazgÃ»l truy Ä‘uá»•i, chiáº¿n Ä‘áº¥u vá»›i lÅ© orc á»Ÿ Moria, vÃ  sá»± pháº£n bá»™i cá»§a Boromir. Sau khi Gandalf ngÃ£ xuá»‘ng á»Ÿ Moria, Há»™i Nháº«n tan rÃ£. Frodo vÃ  Sam tiáº¿p tá»¥c hÃ nh trÃ¬nh má»™t mÃ¬nh, Ä‘Æ°á»£c dáº«n Ä‘Æ°á»ng bá»Ÿi Gollum, má»™t sinh váº­t bá»‹ Ã¡m áº£nh bá»Ÿi chiáº¿c nháº«n.",
# #         #         metadata={"id": 3}
# #         #     ),
# #         #     Document(
# #         #         page_content="Trong khi Ä‘Ã³, Aragorn, Legolas, vÃ  Gimli giÃºp vÆ°Æ¡ng quá»‘c Rohan Roswell vÃ  Gondor chá»‘ng láº¡i quÃ¢n Ä‘á»™i cá»§a Sauron. Sam vÃ  Frodo Ä‘á»‘i máº·t vá»›i nhiá»u thá»­ thÃ¡ch á»Ÿ Mordor, bao gá»“m sá»± cÃ¡m dá»— cá»§a chiáº¿c nháº«n vÃ  sá»± pháº£n bá»™i cá»§a Gollum. Cuá»‘i cÃ¹ng, Frodo phÃ¡ há»§y chiáº¿c nháº«n táº¡i Mount Doom, Ä‘Ã¡nh báº¡i Sauron.",
# #         #         metadata={"id": 4}
# #         #     ),
# #         #     Document(
# #         #         page_content="Tom Bombadil lÃ  má»™t nhÃ¢n váº­t bÃ­ áº©n giÃºp Frodo vÃ  cÃ¡c hobbit thoÃ¡t khá»i nguy hiá»ƒm á»Ÿ Old Forest. Ã”ng khÃ´ng bá»‹ áº£nh hÆ°á»Ÿng bá»Ÿi sá»©c máº¡nh cá»§a chiáº¿c nháº«n. Aragorn, má»™t chiáº¿n binh dÅ©ng cáº£m, lÃ  ngÆ°á»i thá»«a káº¿ ngai vÃ ng Gondor vÃ  dáº«n dáº¯t cÃ¡c dÃ¢n tá»™c tá»± do chá»‘ng láº¡i Sauron.",
# #         #         metadata={"id": 5}
# #         #     )
# #         # ]
# #         # vector_store.add_documents(sample_docs)
# #         # print("ÄÃ£ thÃªm dá»¯ liá»‡u máº«u vá» ChÃºa Nháº«n vÃ o Qdrant.")
# #
# #         return vector_store, qdrant_client
# #
# #     except Exception as e:
# #         print(f"Lá»—i khi thiáº¿t láº­p Qdrant: {str(e)}")
# #         return None, None
# #
# #
# # async def answer_question_with_llama(vector_store, query):
# #     try:
# #         # Khá»Ÿi táº¡o LLM
# #         llm = OllamaLLM(model="llama3.1:8b")
# #
# #         # TÃ¬m kiáº¿m tÆ°Æ¡ng Ä‘á»“ng
# #         docs = vector_store.similarity_search(query, k=5)
# #         context = " ".join([doc.page_content for doc in docs if doc.page_content])
# #         if not context:
# #             return "KhÃ´ng tÃ¬m tháº¥y ná»™i dung liÃªn quan trong Qdrant. Vui lÃ²ng cung cáº¥p thÃªm thÃ´ng tin hoáº·c thá»­ cÃ¢u há»i khÃ¡c."
# #
# #         # Prompt cáº£i tiáº¿n Ä‘á»ƒ tráº£ lá»i chi tiáº¿t
# #         prompt = (
# #             f"Báº¡n lÃ  má»™t trá»£ lÃ½ thÃ´ng minh, am hiá»ƒu vá» 'ChÃºa Nháº«n' cá»§a J.R.R. Tolkien. Dá»±a trÃªn ná»™i dung sau, hÃ£y cung cáº¥p cÃ¢u tráº£ lá»i chi tiáº¿t, Ä‘áº§y Ä‘á»§, vÃ  cÃ³ cáº¥u trÃºc rÃµ rÃ ng (bao gá»“m cÃ¡c Ã½ chÃ­nh, diá»…n biáº¿n quan trá»ng, vÃ  cÃ¡c nhÃ¢n váº­t liÃªn quan). Náº¿u cÃ¢u há»i yÃªu cáº§u tÃ³m táº¯t chi tiáº¿t, hÃ£y liá»‡t kÃª cÃ¡c sá»± kiá»‡n theo thá»© tá»± thá»i gian. Tráº£ lá»i báº±ng tiáº¿ng Viá»‡t, sá»­ dá»¥ng ngÃ´n ngá»¯ tá»± nhiÃªn vÃ  dá»… hiá»ƒu.\n\n"
# #             f"Ná»™i dung:\n{context}\n\n"
# #             f"CÃ¢u há»i: {query}\n\n"
# #             f"Tráº£ lá»i:"
# #         )
# #
# #         # Gá»i LLM
# #         answer = await asyncio.to_thread(llm.invoke, prompt)
# #         return answer
# #
# #     except Exception as e:
# #         return f"Lá»—i khi tráº£ lá»i cÃ¢u há»i: {str(e)}"
# #
# #
# # async def main():
# #     # Thiáº¿t láº­p Qdrant vÃ  vector store
# #     vector_store, qdrant_client = await setup_qdrant()
# #     if not vector_store:
# #         print("KhÃ´ng thá»ƒ tiáº¿p tá»¥c do lá»—i thiáº¿t láº­p Qdrant.")
# #         return
# #
# #     # VÃ²ng láº·p truy váº¥n
# #     while True:
# #         query = input("\nNháº­p cÃ¢u há»i (hoáº·c 'exit' Ä‘á»ƒ thoÃ¡t): ").strip()
# #         if query.lower() in ['exit', 'quit']:
# #             break
# #
# #         result = await answer_question_with_llama(vector_store, query)
# #         print("\nğŸŸ¦ Tráº£ lá»i tá»« LLaMA:\n", result)
# #
# #     # ÄÃ³ng káº¿t ná»‘i Qdrant
# #     if qdrant_client:
# #         qdrant_client.close()
# #
# #
# # if __name__ == "__main__":
# #     asyncio.run(main())
#
#
# import asyncio
# from langchain_community.embeddings import HuggingFaceEmbeddings
# from langchain_qdrant import QdrantVectorStore
# from qdrant_client import QdrantClient
# from qdrant_client.http.models import Distance, VectorParams
# from langchain_core.documents import Document
# from transformers import T5Tokenizer, T5ForConditionalGeneration
# import torch
# import sentencepiece  # Ensure SentencePiece is imported to verify installation
#
# async def setup_qdrant():
#     try:
#         # Initialize embedding model
#         embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
#
#         # Connect to Qdrant
#         qdrant_client = QdrantClient(url="http://localhost:6333", timeout=5)
#         print("Connected to Qdrant successfully!")
#
#         # Check and create collection if it doesn't exist
#         collection_name = "collections"
#         collections = qdrant_client.get_collections()
#         if collection_name not in [c.name for c in collections.collections]:
#             qdrant_client.create_collection(
#                 collection_name=collection_name,
#                 vectors_config=VectorParams(size=384, distance=Distance.COSINE)
#             )
#             print(f"Created collection: {collection_name}")
#
#             # Add sample Harry Potter documents for testing
#             vector_store = QdrantVectorStore(
#                 client=qdrant_client,
#                 collection_name=collection_name,
#                 embedding=embedding_model,
#                 content_payload_key="text"
#             )
#
#         else:
#             vector_store = QdrantVectorStore(
#                 client=qdrant_client,
#                 collection_name=collection_name,
#                 embedding=embedding_model,
#                 content_payload_key="text"
#             )
#
#         return vector_store, qdrant_client
#
#     except Exception as e:
#         print(f"Error setting up Qdrant: {str(e)}")
#         return None, None
#
# async def summarize_with_vit5(vector_store, query):
#     try:
#         # Initialize ViT5
#         model_name = "VietAI/vit5-base"
#         try:
#             tokenizer = T5Tokenizer.from_pretrained(model_name)
#             model = T5ForConditionalGeneration.from_pretrained(model_name)
#         except Exception as e:
#             return f"Error loading ViT5 model or tokenizer: {str(e)}. Ensure SentencePiece is installed (`pip install sentencepiece`)."
#
#         device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
#         model.to(device)
#         print(f"ViT5 model loaded on {device}.")
#
#         # Perform similarity search in Qdrant
#         docs = vector_store.similarity_search(query, k=5)
#         context = " ".join([doc.page_content for doc in docs if doc.page_content])
#         if not context:
#             return "No relevant content found in Qdrant. Please add documents or try a different query."
#
#         # Prompt for summarization
#         prompt = (
#             f"TÃ³m táº¯t cÃ¡c diá»…n biáº¿n chÃ­nh tá»« ná»™i dung sau thÃ nh má»™t Ä‘oáº¡n vÄƒn ngáº¯n gá»n, rÃµ rÃ ng, theo thá»© tá»± thá»i gian, sá»­ dá»¥ng ngÃ´n ngá»¯ tiáº¿ng Viá»‡t tá»± nhiÃªn vÃ  dá»… hiá»ƒu. Táº­p trung vÃ o cÃ¡c sá»± kiá»‡n quan trá»ng vÃ  nhÃ¢n váº­t chÃ­nh liÃªn quan.\n\n"
#             f"Ná»™i dung: {context}\n\n"
#             f"TÃ³m táº¯t:"
#         )
#
#         # Prepare input for ViT5
#         inputs = tokenizer(prompt, return_tensors="pt", max_length=512, truncation=True)
#         inputs = inputs.to(device)
#
#         # Generate summary
#         summary_ids = model.generate(
#             inputs["input_ids"],
#             max_length=150,
#             min_length=50,
#             length_penalty=1.0,
#             num_beams=4,
#             early_stopping=True
#         )
#         summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
#
#         return summary
#
#     except Exception as e:
#         return f"Error during ViT5 summarization: {str(e)}"
#
# async def main():
#     # Set up Qdrant and vector store
#     vector_store, qdrant_client = await setup_qdrant()
#     if not vector_store:
#         print("Cannot proceed due to Qdrant setup error.")
#         return
#
#     # Query loop
#     while True:
#         query = input("\nEnter your query (or 'exit' to quit): ").strip()
#         if query.lower() in ['exit', 'quit']:
#             break
#
#         print("\nProcessing query...")
#         result = await summarize_with_vit5(vector_store, query)
#         print("\nğŸŸ¦ Summary from ViT5:\n", result)
#
#     # Close Qdrant connection
#     if qdrant_client:
#         qdrant_client.close()
#
# if __name__ == "__main__":
#     asyncio.run(main())


from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

# Load model vÃ  tokenizer má»™t láº§n
tokenizer = AutoTokenizer.from_pretrained("VietAI/vit5-large-vietnews-summarization")
model = AutoModelForSeq2SeqLM.from_pretrained("VietAI/vit5-large-vietnews-summarization")
device = "cuda" if model.device.type == "cuda" else "cpu"
model.to(device)

# VÃ²ng láº·p nháº­p input
print("Nháº­p Ä‘oáº¡n vÄƒn báº£n tiáº¿ng Viá»‡t Ä‘á»ƒ tÃ³m táº¯t (gÃµ 'exit' Ä‘á»ƒ thoÃ¡t):\n")
while True:
    sentence = input(">> Nháº­p vÄƒn báº£n: ").strip()
    if sentence.lower() == "exit":
        print("ÄÃ£ thoÃ¡t.")
        break
    if not sentence:
        print("âŒ Vui lÃ²ng khÃ´ng Ä‘á»ƒ trá»‘ng.")
        continue

    # Chuáº©n bá»‹ Ä‘áº§u vÃ o cho model
    text = "vietnews: " + sentence + " </s>"
    encoding = tokenizer(text, return_tensors="pt").to(device)

    # Sinh tÃ³m táº¯t
    outputs = model.generate(
        input_ids=encoding["input_ids"],
        attention_mask=encoding["attention_mask"],
        max_length=256,
        early_stopping=True
    )

    # Hiá»ƒn thá»‹ káº¿t quáº£
    summary = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)
    print("\nğŸ“ TÃ³m táº¯t:")
    print(summary)
    print("-" * 80)


# import torch
# print(torch.cuda.is_available())
